{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6492730-e839-4e6e-89a3-3bcc2ab103b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of train_df:\n",
            "                                                data\n",
            "0  1 a stirring , funny and finally transporting ...\n",
            "1  0 apparently reassembled from the cutting-room...\n",
            "2  0 they presume their audience wo n't sit still...\n",
            "3  1 this is a visually stunning rumination on lo...\n",
            "4  1 jonathan parker 's bartleby should have been...\n",
            "First few rows of test_df:\n",
            "                                                data\n",
            "0   0 no movement , no yuks , not much of anything .\n",
            "1  0 a gob of drivel so sickly sweet , even the e...\n",
            "2  0 gangs of new york is an unapologetic mess , ...\n",
            "3  0 we never really feel involved with the story...\n",
            "4          1 this is one of polanski 's best films .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import xgboost as xgb\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "# Ensure nltk resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the data from local .txt files\n",
        "train_file_path = \"/content/stsa-train.txt\"\n",
        "test_file_path = \"/content/stsa-test.txt\"\n",
        "\n",
        "train_df = pd.read_csv(train_file_path, header=None, sep='\\t', names=['data'])\n",
        "test_df = pd.read_csv(test_file_path, header=None, sep='\\t', names=['data'])\n",
        "\n",
        "print(\"First few rows of train_df:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"First few rows of test_df:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# Separating label and text\n",
        "train_df['label'] = train_df['data'].apply(lambda x: int(x[0]))\n",
        "train_df['text'] = train_df['data'].apply(lambda x: x[2:])\n",
        "test_df['label'] = test_df['data'].apply(lambda x: int(x[0]))\n",
        "test_df['text'] = test_df['data'].apply(lambda x: x[2:])\n",
        "\n",
        "# Drop the original combined column\n",
        "train_df.drop('data', axis=1, inplace=True)\n",
        "test_df.drop('data', axis=1, inplace=True)\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(preprocess)\n",
        "test_df['text'] = test_df['text'].apply(preprocess)\n",
        "\n",
        "# Vectorization with TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(train_df['text'])\n",
        "X_test_tfidf = vectorizer.transform(test_df['text'])\n",
        "\n",
        "# Train a Word2Vec model\n",
        "tokenized_train = [word_tokenize(text) for text in train_df['text']]\n",
        "tokenized_test = [word_tokenize(text) for text in test_df['text']]\n",
        "w2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "def feature_vector(text, model):\n",
        "    words = set(model.wv.index_to_key)\n",
        "    word_vecs = [model.wv[word] for word in text if word in words]\n",
        "    if len(word_vecs) > 0:\n",
        "        return np.mean(word_vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(100)\n",
        "\n",
        "X_train_w2v = np.array([feature_vector(text, w2v_model) for text in tokenized_train])\n",
        "X_test_w2v = np.array([feature_vector(text, w2v_model) for text in tokenized_test])\n",
        "\n",
        "# BERT tokenizer and model loading\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def bert_encode(texts, tokenizer, model, max_len=128):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoded_batch = tokenizer.batch_encode_plus(texts, add_special_tokens=True, max_length=max_len, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids, attention_mask = encoded_batch['input_ids'].to(device), encoded_batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    embeddings = outputs.last_hidden_state[:,0,:].detach().cpu().numpy()  # Take the embeddings from the first token ([CLS] token)\n",
        "    return embeddings\n",
        "\n",
        "X_train_bert = bert_encode(train_df['text'].tolist(), tokenizer, bert_model)\n",
        "X_test_bert = bert_encode(test_df['text'].tolist(), tokenizer, bert_model)\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# Model setup\n",
        "models = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(3),\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    'RandomForest_TFIDF': RandomForestClassifier(),\n",
        "    'RandomForest_Word2Vec': RandomForestClassifier(),\n",
        "    'RandomForest_BERT': RandomForestClassifier()  # This will use BERT embeddings\n",
        "}\n",
        "\n",
        "# Evaluate models using 10-fold cross-validation and on the test set\n",
        "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "for name, model in models.items():\n",
        "    if 'RandomForest' not in name:  # Evaluate using TF-IDF\n",
        "        cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=kf)\n",
        "        print(f\"{name} with TF-IDF - 10-fold CV Accuracy: {cv_scores.mean()}\")\n",
        "        model.fit(X_train_tfidf, y_train)\n",
        "    elif 'TFIDF' in name:\n",
        "        cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=kf)\n",
        "        print(f\"{name} - 10-fold CV Accuracy: {cv_scores.mean()}\")\n",
        "        model.fit(X_train_tfidf, y_train)\n",
        "    elif 'Word2Vec' in name:\n",
        "        cv_scores = cross_val_score(model, X_train_w2v, y_train, cv=kf)\n",
        "        print(f\"{name} - 10-fold CV Accuracy: {cv_scores.mean()}\")\n",
        "        model.fit(X_train_w2v, y_train)\n",
        "    elif 'BERT' in name:\n",
        "        # Assuming BERT embeddings are correctly generated\n",
        "        cv_scores = cross_val_score(model, X_train_bert, y_train, cv=kf)\n",
        "        print(f\"{name} with BERT - 10-fold CV Accuracy: {cv_scores.mean()}\")\n",
        "        model.fit(X_train_bert, y_train)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    if 'Word2Vec' in name:\n",
        "        predictions = model.predict(X_test_w2v)\n",
        "    elif 'BERT' in name:\n",
        "        predictions = model.predict(X_test_bert)\n",
        "    else:\n",
        "        predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(f\"{name} - Test Accuracy: {accuracy_score(y_test, predictions)}\")\n",
        "    print(f\"{name} - Precision: {precision_score(y_test, predictions, average='macro')}\")\n",
        "    print(f\"{name} - Recall: {recall_score(y_test, predictions, average='macro')}\")\n",
        "    print(f\"{name} - F1 Score: {f1_score(y_test, predictions, average='macro')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e4d13b-3d95-4679-9a89-7ca3fad6a1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Shape: (14509, 6)\n",
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0     5.0  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1     4.0  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2     5.0                                       Very pleased           0.0  \n",
            "3     4.0  It works good but it goes slow sometimes but i...           0.0  \n",
            "4     4.0  Great phone to replace my lost phone. The only...           0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Assuming the text data for clustering is in the 'Reviews' column\n",
        "text_data = df['Reviews'].astype(str)\n",
        "\n",
        "# TF-IDF Vectorization for K-means clustering\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=5)\n",
        "kmeans.fit(tfidf_matrix)\n",
        "kmeans_labels = kmeans.labels_\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan.fit(tfidf_matrix)\n",
        "dbscan_labels = dbscan.labels_\n",
        "\n",
        "# Hierarchical clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=5)\n",
        "hierarchical.fit(tfidf_matrix.toarray())\n",
        "hierarchical_labels = hierarchical.labels_\n",
        "\n",
        "# Word2Vec clustering\n",
        "tokenized_text = [review.split() for review in text_data]\n",
        "w2v_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_vectors = [w2v_model.wv[word] for word in w2v_model.wv.index_to_key]\n",
        "kmeans_w2v = KMeans(n_clusters=5)\n",
        "kmeans_w2v.fit(w2v_vectors)\n",
        "w2v_labels = kmeans_w2v.labels_\n",
        "\n",
        "# BERT clustering\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def bert_encode(texts, tokenizer, model):\n",
        "    encoded_batch = tokenizer.batch_encode_plus(texts, add_special_tokens=True, max_length=128, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids, attention_mask = encoded_batch['input_ids'].to(device), encoded_batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    embeddings = outputs.last_hidden_state[:,0,:].detach().cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "bert_vectors = bert_encode(text_data.tolist(), tokenizer, bert_model)\n",
        "kmeans_bert = KMeans(n_clusters=5)\n",
        "kmeans_bert.fit(bert_vectors)\n",
        "bert_labels = kmeans_bert.labels_\n",
        "\n",
        "# Print clustering results\n",
        "print(\"\\nK-means labels:\", kmeans_labels)\n",
        "print(\"DBSCAN labels:\", dbscan_labels)\n",
        "print(\"Hierarchical labels:\", hierarchical_labels)\n",
        "print(\"Word2Vec labels:\", w2v_labels)\n",
        "print(\"BERT labels:\", bert_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "The exercises provide a practical hands-on experience with various text clustering methods, covering a diverse range from traditional techniques like K-means and DBSCAN to advanced approaches like Word2Vec and BERT embeddings. The code is well-structured and easy to follow, making it accessible for beginners in text mining. It's a great starting point for understanding and experimenting with text clustering techniques on real-world datasets.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}